{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ab0771a8",
      "metadata": {},
      "source": [
        "# Build master data + feature-rich model\n",
        "Single notebook to build master_data and engineer richer features for EC/pH/soil temp forecasting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "84ac937b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from datetime import datetime, timedelta, time\n",
        "import csv\n",
        "from typing import Any, Dict, List, Optional\n",
        "from openpyxl import load_workbook\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1cafa46f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building master_data ...\n",
            "Wrote master_data.csv with 16682 rows\n"
          ]
        }
      ],
      "source": [
        "# --------------------\n",
        "# Data build utilities\n",
        "# --------------------\n",
        "\n",
        "def floor_to_10min(dt: datetime) -> datetime:\n",
        "    minute = (dt.minute // 10) * 10\n",
        "    return dt.replace(minute=minute, second=0, microsecond=0)\n",
        "\n",
        "def parse_time_flex(text: str) -> time:\n",
        "    text = text.strip()\n",
        "    parts = text.split(\":\")\n",
        "    if len(parts) >= 2:\n",
        "        try:\n",
        "            h = int(parts[0]); m = int(parts[1]); s = int(parts[2]) if len(parts) >= 3 and parts[2] else 0\n",
        "            return time(hour=h, minute=m, second=s)\n",
        "        except Exception:\n",
        "            pass\n",
        "    for fmt in (\"%H:%M:%S\", \"%H:%M\"):\n",
        "        try:\n",
        "            return datetime.strptime(text, fmt).time()\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise ValueError(f\"Unrecognized time format: {text}\")\n",
        "\n",
        "def parse_ts_variant(text: str, day_first: bool) -> datetime:\n",
        "    norm = text.replace(\"-\", \"/\").replace(\".\", \"/\").strip()\n",
        "    if \" \" not in norm:\n",
        "        raise ValueError(\"No space between date and time\")\n",
        "    date_part, time_part = norm.split(\" \", 1)\n",
        "    t_val = parse_time_flex(time_part)\n",
        "    parts = date_part.split(\"/\")\n",
        "    if len(parts) < 3:\n",
        "        raise ValueError(\"Date part incomplete\")\n",
        "    a, b, y = parts[0], parts[1], parts[2]\n",
        "    if len(y) == 2:\n",
        "        y = \"20\" + y\n",
        "    if day_first:\n",
        "        day, month = int(a), int(b)\n",
        "    else:\n",
        "        month, day = int(a), int(b)\n",
        "    return datetime(int(y), month, day, t_val.hour, t_val.minute, t_val.second)\n",
        "\n",
        "def parse_micro_ts(value: Any, prev_ts: Optional[datetime]) -> datetime:\n",
        "    if isinstance(value, datetime):\n",
        "        ts = value\n",
        "    else:\n",
        "        text = str(value).strip()\n",
        "        candidates = []\n",
        "        for day_first in (True, False):\n",
        "            try:\n",
        "                candidates.append(parse_ts_variant(text, day_first))\n",
        "            except Exception:\n",
        "                continue\n",
        "        if not candidates:\n",
        "            raise ValueError(f\"Unrecognized micro-climate timestamp: {value}\")\n",
        "        if prev_ts is not None:\n",
        "            best = None; best_delta = None\n",
        "            for dt in candidates:\n",
        "                delta = dt - prev_ts\n",
        "                if delta >= timedelta(minutes=-5) and (best_delta is None or delta < best_delta):\n",
        "                    best = dt; best_delta = delta\n",
        "            ts = best if best is not None else candidates[0]\n",
        "        else:\n",
        "            ts = candidates[0]\n",
        "    return ts\n",
        "\n",
        "def parse_time_robust(val: Any) -> time:\n",
        "    if isinstance(val, datetime):\n",
        "        return val.time()\n",
        "    if isinstance(val, time):\n",
        "        return val\n",
        "    return parse_time_flex(str(val))\n",
        "\n",
        "def parse_date_dayfirst(text: str) -> datetime.date:\n",
        "    text = text.strip()\n",
        "    norm = text.replace(\"-\", \"/\").replace(\".\", \"/\")\n",
        "    parts = norm.split(\"/\")\n",
        "    if len(parts) >= 3:\n",
        "        d, m, y = parts[0], parts[1], parts[2]\n",
        "        if len(y) == 2:\n",
        "            y = \"20\" + y\n",
        "        return datetime(int(y), int(m), int(d)).date()\n",
        "    for fmt in (\"%d/%m/%Y\", \"%d/%m/%y\", \"%Y-%m-%d\"):\n",
        "        try:\n",
        "            return datetime.strptime(text, fmt).date()\n",
        "        except Exception:\n",
        "            continue\n",
        "    raise ValueError(f\"Unrecognized date format: {text}\")\n",
        "\n",
        "def read_micro(path: str) -> List[Dict[str, Any]]:\n",
        "    wb = load_workbook(path, read_only=True, data_only=True)\n",
        "    ws = wb[\"micro_climate_rh_t_et0\"]\n",
        "    rows = ws.iter_rows(values_only=True)\n",
        "    next(rows)\n",
        "    data = []\n",
        "    prev_ts = None\n",
        "    base_ts = None\n",
        "    interval = timedelta(minutes=10)\n",
        "    for row in rows:\n",
        "        if not row or row[0] is None:\n",
        "            continue\n",
        "        raw_ts = parse_micro_ts(row[0], prev_ts)\n",
        "        if base_ts is None:\n",
        "            base_ts = raw_ts\n",
        "            seq_ts = base_ts\n",
        "        else:\n",
        "            seq_ts = base_ts + interval * len(data)\n",
        "        prev_ts = raw_ts\n",
        "        data.append({\n",
        "            \"timestamp\": seq_ts,\n",
        "            \"ET0\": row[1],\n",
        "            \"internal_air_temp_c\": row[2],\n",
        "            \"internal_rh_%\": row[3],\n",
        "            \"internal_radiation\": row[4],\n",
        "        })\n",
        "    return data\n",
        "\n",
        "def read_irrigation(path: str):\n",
        "    wb = load_workbook(path, read_only=True, data_only=True)\n",
        "    ws = wb.active\n",
        "    rows = ws.iter_rows(values_only=True)\n",
        "    next(rows)\n",
        "    events = []\n",
        "    for row in rows:\n",
        "        if not row or row[0] is None:\n",
        "            continue\n",
        "        date_val, time_val, irr_ml, fert_type = row[0], row[1], row[2], row[3]\n",
        "        if isinstance(date_val, datetime):\n",
        "            date_only = date_val.date()\n",
        "        else:\n",
        "            parsed = None\n",
        "            for fmt in (\"%Y-%m-%d\", \"%d/%m/%Y\", \"%m/%d/%Y\", \"%d.%m.%Y\", \"%m.%d.%Y\"):\n",
        "                try:\n",
        "                    parsed = datetime.strptime(str(date_val), fmt).date(); break\n",
        "                except Exception:\n",
        "                    continue\n",
        "            if parsed is None:\n",
        "                parsed = parse_date_dayfirst(str(date_val))\n",
        "            date_only = parsed\n",
        "        t_val = parse_time_robust(time_val)\n",
        "        dt = datetime.combine(date_only, t_val)\n",
        "        bucket = floor_to_10min(dt)\n",
        "        fert_code = 0\n",
        "        if isinstance(fert_type, str):\n",
        "            f = fert_type.strip().upper()\n",
        "            if f == \"A\":\n",
        "                fert_code = 1\n",
        "            elif f == \"B\":\n",
        "                fert_code = 2\n",
        "        events.append({\"bucket\": bucket, \"irr_ml\": irr_ml or 0.0, \"fert_code\": fert_code})\n",
        "    return events\n",
        "\n",
        "def build_irrigation_series(events):\n",
        "    fert_by_bucket = {}\n",
        "    irr_by_bucket = defaultdict(float)\n",
        "    for ev in events:\n",
        "        irr_by_bucket[ev[\"bucket\"]] += ev[\"irr_ml\"]\n",
        "        if ev[\"fert_code\"]:\n",
        "            fert_by_bucket[ev[\"bucket\"]] = max(fert_by_bucket.get(ev[\"bucket\"], 0), ev[\"fert_code\"])\n",
        "    return irr_by_bucket, fert_by_bucket\n",
        "\n",
        "def parse_ph_ec_date(val: Any) -> Optional[datetime.date]:\n",
        "    if isinstance(val, datetime):\n",
        "        return val.date()\n",
        "    if val is None:\n",
        "        return None\n",
        "    s = str(val).strip()\n",
        "    if not s:\n",
        "        return None\n",
        "    for sep in (\".\", \"/\", \"-\"):\n",
        "        if sep in s:\n",
        "            parts = s.replace(\"-\", sep).replace(\"/\", sep).split(sep)\n",
        "            if len(parts) >= 3:\n",
        "                d_raw, m_raw, y_raw = parts[0], parts[1], parts[2]\n",
        "                for day_first in (True, False):\n",
        "                    day, month = (d_raw, m_raw) if day_first else (m_raw, d_raw)\n",
        "                    year = y_raw\n",
        "                    if len(year) == 2:\n",
        "                        year = \"20\" + year\n",
        "                    try:\n",
        "                        return datetime(int(year), int(month), int(day)).date()\n",
        "                    except Exception:\n",
        "                        continue\n",
        "    for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%m.%d.%Y\", \"%d.%m.%Y\", \"%m-%d-%Y\", \"%d-%m-%Y\"):\n",
        "        try:\n",
        "            return datetime.strptime(s, fmt).date()\n",
        "        except Exception:\n",
        "            continue\n",
        "    return None\n",
        "\n",
        "def read_ph_ec(path: str):\n",
        "    wb = load_workbook(path, read_only=True, data_only=True)\n",
        "    ws = wb.active\n",
        "    rows = ws.iter_rows(values_only=True)\n",
        "    next(rows)\n",
        "    measurements = []\n",
        "    for row in rows:\n",
        "        if not row or row[1] is None:\n",
        "            continue\n",
        "        date_part = parse_ph_ec_date(row[1])\n",
        "        time_part = row[4]\n",
        "        if date_part is None or time_part is None:\n",
        "            continue\n",
        "        t_val = parse_time_robust(time_part)\n",
        "        dt = datetime.combine(date_part, t_val)\n",
        "        measurements.append({\"timestamp\": dt, \"ph\": row[2], \"ec_ms\": row[3]})\n",
        "    return measurements\n",
        "\n",
        "def map_ph_ec_to_micro(timestamps: List[datetime], measurements: List[Dict[str, Any]]):\n",
        "    ph_list = [None] * len(timestamps)\n",
        "    ec_list = [None] * len(timestamps)\n",
        "    for m in measurements:\n",
        "        best_idx = None; best_diff = None\n",
        "        for idx, ts in enumerate(timestamps):\n",
        "            diff = abs(ts - m[\"timestamp\"])\n",
        "            if diff <= timedelta(minutes=10) and (best_diff is None or diff < best_diff):\n",
        "                best_idx = idx; best_diff = diff\n",
        "        if best_idx is not None:\n",
        "            ph_list[best_idx] = m[\"ph\"]\n",
        "            ec_list[best_idx] = m[\"ec_ms\"]\n",
        "    return ph_list, ec_list\n",
        "\n",
        "def read_soil_temp(path: str):\n",
        "    data = {}\n",
        "    with open(path, newline=\"\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for row in reader:\n",
        "            ts = datetime.strptime(row[\"datetime\"], \"%Y-%m-%d %H:%M:%S\")\n",
        "            data[ts] = float(row[\"soil_temp_pred\"]) if row[\"soil_temp_pred\"] != \"\" else None\n",
        "    return data\n",
        "\n",
        "def read_canopy(path: str):\n",
        "    wb = load_workbook(path, read_only=True, data_only=True)\n",
        "    ws = wb.active\n",
        "    rows = ws.iter_rows(values_only=True)\n",
        "    next(rows)\n",
        "    canopy = {}\n",
        "    for row in rows:\n",
        "        if len(row) < 7:\n",
        "            continue\n",
        "        date_val = row[4]\n",
        "        canopy_val = row[6]\n",
        "        if date_val is None:\n",
        "            continue\n",
        "        if not isinstance(date_val, datetime):\n",
        "            try:\n",
        "                date_val = datetime.strptime(str(date_val), \"%Y-%m-%d\")\n",
        "            except Exception:\n",
        "                continue\n",
        "        canopy[date_val.date()] = canopy_val\n",
        "    return canopy\n",
        "\n",
        "def build_master():\n",
        "    micro = read_micro(\"micro_climate_rh_t_et0.xlsx\")\n",
        "    events = read_irrigation(\"Irrigation + ALL Elemental Fractions schedule for one plant (100N).xlsx\")\n",
        "    irr_by_bucket, fert_by_bucket = build_irrigation_series(events)\n",
        "    ph_ec = read_ph_ec(\"PH+EC Final.xlsx\")\n",
        "    soil_temp = read_soil_temp(\"soil_temp_predictions_full_range.csv\")\n",
        "    canopy = read_canopy(\"Daily Canopy Cover Values.xlsx\")\n",
        "\n",
        "    timestamps = [row[\"timestamp\"] for row in micro]\n",
        "    ph_list, ec_list = map_ph_ec_to_micro(timestamps, ph_ec)\n",
        "\n",
        "    irr_values = []\n",
        "    irrigation_3h = []\n",
        "    fert_flags = []\n",
        "    soil_list = []\n",
        "    canopy_list = []\n",
        "    irr_current = []\n",
        "\n",
        "    for i, ts in enumerate(timestamps):\n",
        "        irr_now = irr_by_bucket.get(ts, 0.0)\n",
        "        irr_current.append(irr_now)\n",
        "        irr_values.append(irr_now)\n",
        "        window_sum = sum(irr_values[max(0, i - 17): i + 1])\n",
        "        irrigation_3h.append(window_sum)\n",
        "        fert_flags.append(fert_by_bucket.get(ts, 0))\n",
        "        soil_list.append(soil_temp.get(ts))\n",
        "        canopy_list.append(canopy.get(ts.date()))\n",
        "\n",
        "    df_out = pd.DataFrame({\n",
        "        \"timestamp\": timestamps,\n",
        "        \"ET0\": [r[\"ET0\"] for r in micro],\n",
        "        \"internal_air_temp_c\": [r[\"internal_air_temp_c\"] for r in micro],\n",
        "        \"internal_rh_%\": [r[\"internal_rh_%\"] for r in micro],\n",
        "        \"internal_radiation\": [r[\"internal_radiation\"] for r in micro],\n",
        "        \"irrigation_ml_current\": irr_current,\n",
        "        \"irrigation_last_3h_ml\": irrigation_3h,\n",
        "        \"fertilization_flag\": fert_flags,\n",
        "        \"ph\": ph_list,\n",
        "        \"ec_ms\": ec_list,\n",
        "        \"soil_temp_pred\": soil_list,\n",
        "        \"canopy_cover\": canopy_list,\n",
        "    })\n",
        "    df_out = df_out.sort_values(\"timestamp\").set_index(\"timestamp\")\n",
        "    out_path = Path(\"master_data.csv\")\n",
        "    df_out.to_csv(out_path, index=True)\n",
        "    print(f\"Wrote {out_path} with {len(df_out)} rows\")\n",
        "    return df_out\n",
        "\n",
        "print(\"Building master_data ...\")\n",
        "df = build_master()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5285c256",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Built ds with intervals: 108\n"
          ]
        }
      ],
      "source": [
        "# 2b) Build ds from master_data anchor-to-anchor intervals\n",
        "master = pd.read_csv('master_data.csv', parse_dates=['timestamp'])\n",
        "master = master.sort_values('timestamp')\n",
        "anchors = master[master['ph'].notna() & master['ec_ms'].notna()].copy()\n",
        "anchors['t1'] = anchors['timestamp'].shift(-1)\n",
        "anchors['ph1'] = anchors['ph'].shift(-1)\n",
        "anchors['ec1'] = anchors['ec_ms'].shift(-1)\n",
        "anchors['gap_hours'] = (anchors['t1'] - anchors['timestamp']).dt.total_seconds()/3600\n",
        "anchors['dph'] = anchors['ph1'] - anchors['ph']\n",
        "anchors['dec'] = anchors['ec1'] - anchors['ec_ms']\n",
        "# drop last anchor without next\n",
        "anchors = anchors.dropna(subset=['t1','ph1','ec1','dph','dec'])\n",
        "# Feature set: current obs and context\n",
        "ds = anchors[[\n",
        "    'timestamp','t1','gap_hours',\n",
        "    'ph','ec_ms','soil_temp_pred','internal_air_temp_c','internal_rh_%',\n",
        "    'internal_radiation','ET0','irrigation_last_3h_ml','fertilization_flag','canopy_cover',\n",
        "    'dph','dec'\n",
        "]].copy()\n",
        "ds = ds.rename(columns={'ph':'ph0','ec_ms':'ec0'})\n",
        "ds = ds.set_index('timestamp')\n",
        "print('Built ds with intervals:', len(ds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "496029d1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tested intervals (no skips): 78 out of total: 108 | first test index: 30\n",
            "\n",
            "=== Overall next-observation performance (ALL tested intervals) ===\n",
            "PH MAE: 0.7234278396392958 RMSE: 0.9270640628418901 R2: 0.5792511094588813\n",
            "EC MAE: 0.3010996305631767 RMSE: 0.5839816139274692 R2: 0.7543452071085192\n",
            "\n",
            "=== Relative Error (%) (ALL tested intervals) ===\n",
            "PH Mean RE (%): 8.845067982498154 Median RE (%): 6.881928666722287\n",
            "EC Mean RE (%): 64.3659351516853 Median RE (%): 29.190954827521047\n",
            "\n",
            "EC range counts (tested):\n",
            "ec1_true\n",
            "0.3–1    41\n",
            "<0.3     24\n",
            "1–3       7\n",
            ">3        6\n",
            "Name: count, dtype: int64\n",
            "EC range 1–3: n=7 Mean RE (%): 21.210 Median RE (%): 24.146\n",
            "EC range 0.3–1: n=41 Mean RE (%): 46.058 Median RE (%): 19.946\n",
            "EC range <0.3: n=24 Mean RE (%): 117.556 Median RE (%): 73.082\n",
            "EC range >3: n=6 Mean RE (%): 27.058 Median RE (%): 16.713\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def rmse(a, b):\n",
        "    return float(np.sqrt(mean_squared_error(a, b)))\n",
        "\n",
        "def relative_error_percent(y_true, y_pred, eps=1e-6):\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_pred = np.asarray(y_pred)\n",
        "    return np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), eps) * 100\n",
        "\n",
        "# ============================================================\n",
        "# 4) Walk-forward (test every interval, no skips)\n",
        "# ============================================================\n",
        "feature_cols = [c for c in ds.columns if c not in (\"dph\",\"dec\",\"t1\")]\n",
        "X_all = ds[feature_cols].copy()\n",
        "y_all = ds[[\"dph\",\"dec\"]].copy()\n",
        "\n",
        "initial_train = 30\n",
        "\n",
        "if len(ds) <= initial_train + 1:\n",
        "    raise RuntimeError(f\"Not enough intervals: n={len(ds)} for initial_train={initial_train}\")\n",
        "\n",
        "pred_rows = []\n",
        "\n",
        "# Loop over every next interval from initial_train to end-1\n",
        "for i in range(initial_train, len(ds)):\n",
        "    X_train = X_all.iloc[:i]\n",
        "    y_train = y_all.iloc[:i]\n",
        "\n",
        "    X_test  = X_all.iloc[i:i+1]\n",
        "    y_test  = y_all.iloc[i:i+1]\n",
        "\n",
        "    base_rf = RandomForestRegressor(\n",
        "        n_estimators=800,\n",
        "        max_depth=8,\n",
        "        min_samples_leaf=4,\n",
        "        min_samples_split=10,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    model = MultiOutputRegressor(base_rf)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    pred = model.predict(X_test)\n",
        "    dph_pred, dec_pred = float(pred[0, 0]), float(pred[0, 1])\n",
        "\n",
        "    ph0 = float(ds.iloc[i][\"ph0\"])\n",
        "    ec0 = float(ds.iloc[i][\"ec0\"])\n",
        "\n",
        "    dph_true = float(y_test[\"dph\"].iloc[0])\n",
        "    dec_true = float(y_test[\"dec\"].iloc[0])\n",
        "\n",
        "    ph1_true = ph0 + dph_true\n",
        "    ec1_true = ec0 + dec_true\n",
        "\n",
        "    ph1_pred = ph0 + dph_pred\n",
        "    ec1_pred = ec0 + dec_pred\n",
        "\n",
        "    pred_rows.append({\n",
        "        \"t0\": ds.index[i],\n",
        "        \"t1\": ds.iloc[i][\"t1\"],\n",
        "        \"gap_hours\": float(ds.iloc[i][\"gap_hours\"]),\n",
        "        \"ph0\": ph0, \"ec0\": ec0,\n",
        "        \"dph_true\": dph_true, \"dec_true\": dec_true,\n",
        "        \"dph_pred\": dph_pred, \"dec_pred\": dec_pred,\n",
        "        \"ph1_true\": ph1_true, \"ec1_true\": ec1_true,\n",
        "        \"ph1_pred\": ph1_pred, \"ec1_pred\": ec1_pred,\n",
        "        \"train_intervals\": i\n",
        "    })\n",
        "\n",
        "pred_df = pd.DataFrame(pred_rows).set_index(\"t0\").sort_index()\n",
        "\n",
        "print(\"Tested intervals (no skips):\", len(pred_df), \"out of total:\", len(ds),\n",
        "      \"| first test index:\", initial_train)\n",
        "\n",
        "# ============================================================\n",
        "# 5) Overall metrics on ALL tested intervals\n",
        "# ============================================================\n",
        "ph1_true = pred_df[\"ph1_true\"].values\n",
        "ph1_pred = pred_df[\"ph1_pred\"].values\n",
        "ec1_true = pred_df[\"ec1_true\"].values\n",
        "ec1_pred = pred_df[\"ec1_pred\"].values\n",
        "\n",
        "print(\"\\n=== Overall next-observation performance (ALL tested intervals) ===\")\n",
        "print(\"PH MAE:\", float(mean_absolute_error(ph1_true, ph1_pred)),\n",
        "      \"RMSE:\", rmse(ph1_true, ph1_pred),\n",
        "      \"R2:\", float(r2_score(ph1_true, ph1_pred)))\n",
        "print(\"EC MAE:\", float(mean_absolute_error(ec1_true, ec1_pred)),\n",
        "      \"RMSE:\", rmse(ec1_true, ec1_pred),\n",
        "      \"R2:\", float(r2_score(ec1_true, ec1_pred)))\n",
        "\n",
        "# Relative error\n",
        "ph_rel = relative_error_percent(ph1_true, ph1_pred)\n",
        "ec_rel = relative_error_percent(ec1_true, ec1_pred)\n",
        "print(\"\\n=== Relative Error (%) (ALL tested intervals) ===\")\n",
        "print(\"PH Mean RE (%):\", float(ph_rel.mean()), \"Median RE (%):\", float(np.median(ph_rel)))\n",
        "print(\"EC Mean RE (%):\", float(ec_rel.mean()), \"Median RE (%):\", float(np.median(ec_rel)))\n",
        "\n",
        "# ============================================================\n",
        "# 6) EC range breakdown\n",
        "# ============================================================\n",
        "ec_bins = pd.cut(\n",
        "    pred_df[\"ec1_true\"],\n",
        "    bins=[0, 0.3, 1.0, 3.0, 10],\n",
        "    labels=[\"<0.3\", \"0.3–1\", \"1–3\", \">3\"]\n",
        ")\n",
        "print(\"\\nEC range counts (tested):\")\n",
        "print(ec_bins.value_counts(dropna=False))\n",
        "\n",
        "for b in ec_bins.dropna().unique():\n",
        "    mask = (ec_bins == b).values\n",
        "    print(f\"EC range {b}: n={mask.sum()} Mean RE (%): {ec_rel[mask].mean():.3f} Median RE (%): {np.median(ec_rel[mask]):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "88745724",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved: rf_rootzone_model.csv\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 7) Save predictions\n",
        "# ============================================================\n",
        "pred_df.to_csv(\"rf_rootzone_model.csv\")\n",
        "print(\"\\nSaved: rf_rootzone_model.csv\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf-cpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
