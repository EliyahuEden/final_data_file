{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# RH/Temp/ETo XGBoost (outside-only)\n",
    "\n",
    "Single-model per target using outside weather/radiation features with time-based train/test split.\n",
    "\n",
    "Internal air temperature now comes from the `air_temp_C` column inside `rh_et0.csv` (no `air_temp_c.csv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "\n",
    "DATA_DIR = Path(__file__).resolve().parent if \"__file__\" in globals() else Path().resolve()\n",
    "HOURS_LAGS = [1, 2, 3, 6, 12, 24]\n",
    "HOURS_ROLL = [1, 2, 6, 12, 24]\n",
    "TEST_FRAC = 0.2\n",
    "MAX_LAG_STEPS = 24  # cap lags/rolls to avoid very long histories (approx <=4h at 10min freq)\n",
    "TIMESTAMP_COL = \"timestamp\"\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "loaders",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\edene\\AppData\\Local\\Temp\\ipykernel_1692\\606571614.py:10: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  wx = wx.replace(\"-\", np.nan)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load outside weather\n",
    "rad = pd.read_csv(DATA_DIR / \"bet_dagan_radiation.csv\")\n",
    "wx = pd.read_csv(DATA_DIR / \"bet_dagan_weather.csv\")\n",
    "rc = rad.columns.tolist(); wc = wx.columns.tolist()\n",
    "rad = rad.rename(columns={rc[0]: \"station_radiation\", rc[1]: \"datetime\", rc[2]: \"diffuse_radiation_Wm2\", rc[3]: \"global_radiation_Wm2\", rc[4]: \"direct_radiation_Wm2\"})\n",
    "wx = wx.rename(columns={wc[0]: \"station_weather\", wc[1]: \"datetime\", wc[2]: \"station_pressure_hpa\", wc[3]: \"relative_humidity_pct\", wc[4]: \"air_temp_C_weather\", wc[5]: \"air_temp_max_C_weather\", wc[6]: \"air_temp_min_C_weather\", wc[7]: \"ground_temp_C\", wc[8]: \"wet_temp_C\", wc[9]: \"wind_dir_deg\", wc[10]: \"gust_dir_deg\", wc[11]: \"wind_speed_ms\", wc[12]: \"wind_speed_1m_max_ms\", wc[13]: \"wind_speed_10m_max_ms\", wc[14]: \"wind_speed_10m_max_end_time\", wc[15]: \"gust_speed_ms\", wc[16]: \"wind_dir_std_deg\"})\n",
    "rad[\"datetime\"] = pd.to_datetime(rad[\"datetime\"], dayfirst=True)\n",
    "wx[\"datetime\"] = pd.to_datetime(wx[\"datetime\"], dayfirst=True)\n",
    "rad = rad.replace(\"-\", np.nan)\n",
    "wx = wx.replace(\"-\", np.nan)\n",
    "def _coerce(df, ignore):\n",
    "    for c in df.columns:\n",
    "        if c in ignore:\n",
    "            continue\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "rad = _coerce(rad, [\"station_radiation\", \"datetime\"])\n",
    "wx = _coerce(wx, [\"station_weather\", \"datetime\", \"wind_speed_10m_max_end_time\"])\n",
    "outside = pd.merge(rad, wx, on=\"datetime\", how=\"inner\").sort_values(\"datetime\")\n",
    "\n",
    "# Deltas\n",
    "for col in [\"air_temp_C_weather\", \"relative_humidity_pct\", \"global_radiation_Wm2\", \"wind_speed_ms\"]:\n",
    "    outside[f\"{col}_diff\"] = outside[col].diff()\n",
    "\n",
    "# Calendar/time features\n",
    "outside[\"hour\"] = outside[\"datetime\"].dt.hour\n",
    "outside[\"dayofyear\"] = outside[\"datetime\"].dt.dayofyear\n",
    "outside[\"weekofyear\"] = outside[\"datetime\"].dt.isocalendar().week.astype(int)\n",
    "outside[\"dayofyear_sin\"] = np.sin(2 * np.pi * outside[\"dayofyear\"] / 365.25)\n",
    "outside[\"dayofyear_cos\"] = np.cos(2 * np.pi * outside[\"dayofyear\"] / 365.25)\n",
    "outside[\"hour_sin\"] = np.sin(2 * np.pi * outside[\"hour\"] / 24)\n",
    "outside[\"hour_cos\"] = np.cos(2 * np.pi * outside[\"hour\"] / 24)\n",
    "\n",
    "# Interactions\n",
    "outside[\"temp_rad_interaction\"] = outside[\"air_temp_C_weather\"] * outside[\"global_radiation_Wm2\"]\n",
    "outside[\"humidity_wind_interaction\"] = outside[\"relative_humidity_pct\"] * outside[\"wind_speed_ms\"]\n",
    "\n",
    "# Moisture/thermal derived features\n",
    "_a, _b = 17.27, 237.7\n",
    "rh_clip = outside[\"relative_humidity_pct\"].clip(1, 100)\n",
    "gamma = (_a * outside[\"air_temp_C_weather\"] / (_b + outside[\"air_temp_C_weather\"])) + np.log(rh_clip / 100.0)\n",
    "outside[\"dew_point_C\"] = (_b * gamma) / (_a - gamma)\n",
    "es = 0.6108 * np.exp((17.27 * outside[\"air_temp_C_weather\"]) / (outside[\"air_temp_C_weather\"] + 237.3))\n",
    "ea = es * rh_clip / 100.0\n",
    "outside[\"vpd_kpa\"] = es - ea\n",
    "\n",
    "# Load targets (internal air temp + ETo) from rh_et0.csv\n",
    "rh_eto = pd.read_csv(DATA_DIR / \"rh_et0.csv\")\n",
    "rh_eto[\"Date & Time (Summer)\"] = rh_eto[\"Date & Time (Summer)\"].astype(str).str.strip()\n",
    "rh_eto[TIMESTAMP_COL] = pd.to_datetime(rh_eto[\"Date & Time (Summer)\"], dayfirst=True, errors=\"coerce\")\n",
    "rh_eto = rh_eto.rename(columns={\"Eto (mm)\": \"Eto_mm\", \"RH%\": \"Rh_internal\", \"internal_RH\": \"Rh_internal\", \"air_temp_C\": \"air_temp_C\"})\n",
    "for col in [\"Rh_internal\", \"Eto_mm\", \"air_temp_C\"]:\n",
    "    rh_eto[col] = pd.to_numeric(rh_eto[col], errors=\"coerce\")\n",
    "rh_eto = rh_eto[[TIMESTAMP_COL, \"Rh_internal\", \"Eto_mm\", \"air_temp_C\"]].sort_values(TIMESTAMP_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "outside_vars = [\n",
    "    \"air_temp_C_weather\", \"relative_humidity_pct\", \"global_radiation_Wm2\",\n",
    "    \"diffuse_radiation_Wm2\", \"direct_radiation_Wm2\", \"station_pressure_hpa\",\n",
    "    \"wind_speed_ms\", \"gust_speed_ms\", \"dew_point_C\", \"vpd_kpa\",\n",
    "    \"air_temp_C_weather_diff\", \"relative_humidity_pct_diff\", \"global_radiation_Wm2_diff\", \"wind_speed_ms_diff\",\n",
    "    \"hour\", \"dayofyear\", \"weekofyear\", \"dayofyear_sin\", \"dayofyear_cos\", \"hour_sin\", \"hour_cos\",\n",
    "    \"temp_rad_interaction\", \"humidity_wind_interaction\",\n",
    "]\n",
    "\n",
    "def merge_target_with_weather(target_df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
    "    merged = pd.merge_asof(\n",
    "        target_df.sort_values(TIMESTAMP_COL),\n",
    "        outside.sort_values(\"datetime\"),\n",
    "        left_on=TIMESTAMP_COL,\n",
    "        right_on=\"datetime\",\n",
    "        direction=\"nearest\",\n",
    "        tolerance=pd.Timedelta(\"15min\"),\n",
    "    )\n",
    "    merged = merged.drop(columns=[\"datetime\"], errors=\"ignore\")\n",
    "    merged[TIMESTAMP_COL] = pd.to_datetime(merged[TIMESTAMP_COL], errors=\"coerce\")\n",
    "    exclude_start = pd.Timestamp(\"2025-09-17\")\n",
    "    exclude_end = pd.Timestamp(\"2025-09-20\")\n",
    "    merged = merged[(merged[TIMESTAMP_COL] < exclude_start) | (merged[TIMESTAMP_COL] >= exclude_end)]\n",
    "    merged[outside_vars] = merged[outside_vars].ffill().bfill()\n",
    "    merged[target_col] = pd.to_numeric(merged[target_col], errors=\"coerce\")\n",
    "    merged = merged.dropna(subset=[target_col]).reset_index(drop=True)\n",
    "    return merged\n",
    "\n",
    "def build_lagged(\n",
    "    df: pd.DataFrame,\n",
    "    target_col: str,\n",
    "    include_target_lags: bool = False,\n",
    "    feature_vars: List[str] | None = None,\n",
    ") -> Tuple[pd.DataFrame, pd.Series, pd.Series]:\n",
    "    vars_all = (feature_vars or outside_vars) + ([target_col] if include_target_lags else [])\n",
    "    base = df.copy()\n",
    "    ts_sorted = base[TIMESTAMP_COL].sort_values()\n",
    "    if len(ts_sorted) >= 2:\n",
    "        freq_min = ts_sorted.diff().dropna().median().total_seconds() / 60.0\n",
    "        steps_per_hour = max(1, int(round(60.0 / freq_min))) if pd.notnull(freq_min) else 6\n",
    "    else:\n",
    "        steps_per_hour = 6\n",
    "    lag_steps = sorted({max(1, int(round(h * steps_per_hour))) for h in HOURS_LAGS})\n",
    "    roll_windows = sorted({max(1, int(round(h * steps_per_hour))) for h in HOURS_ROLL})\n",
    "    lag_steps = [s for s in lag_steps if s <= MAX_LAG_STEPS]\n",
    "    roll_windows = [s for s in roll_windows if s <= MAX_LAG_STEPS]\n",
    "    no_lag_vars = {\"hour_sin\", \"hour_cos\", \"hour\", \"dayofyear\", \"weekofyear\"}\n",
    "\n",
    "    feat_frames: List[pd.Series] = []\n",
    "    for var in vars_all:\n",
    "        if var in no_lag_vars:\n",
    "            feat_frames.append(base[var].rename(f\"{var}_base\"))\n",
    "            continue\n",
    "        for lag in lag_steps:\n",
    "            feat_frames.append(base[var].shift(lag).rename(f\"{var}_lag_{lag}\"))\n",
    "        for win in roll_windows:\n",
    "            feat_frames.append(base[var].rolling(win).mean().shift(1).rename(f\"{var}_roll_{win}\"))\n",
    "    lagged = pd.concat([base[[TIMESTAMP_COL, target_col]]] + feat_frames, axis=1)\n",
    "    lagged = lagged.dropna().reset_index(drop=True)\n",
    "    X = lagged[[c for c in lagged.columns if c.endswith(tuple([f\"_lag_{l}\" for l in lag_steps])) or \"_roll_\" in c or c.endswith(\"_base\")]]\n",
    "    y = lagged[target_col]\n",
    "    ts = lagged[TIMESTAMP_COL]\n",
    "    return X, y, ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "train_eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics (per target, train/test):\n",
      "- air_temp_C: train R2=0.9977 MAE=0.1766 | test R2=0.9729 MAE=0.5925 | train_rows=3011, test_rows=753\n",
      "- Eto_mm: train R2=0.9845 MAE=0.0140 | test R2=0.9486 MAE=0.0195 | train_rows=4262, test_rows=1066\n",
      "Top correlations with target (train set, abs sorted):\n",
      "- air_temp_C:\n",
      "air_temp_C_weather_lag_1        0.887985\n",
      "temp_rad_interaction_lag_1      0.882814\n",
      "global_radiation_Wm2_lag_1      0.874866\n",
      "temp_rad_interaction_lag_2      0.872739\n",
      "air_temp_C_weather_lag_2        0.869311\n",
      "global_radiation_Wm2_lag_2      0.866829\n",
      "temp_rad_interaction_lag_3      0.860396\n",
      "temp_rad_interaction_roll_6     0.859891\n",
      "global_radiation_Wm2_roll_6     0.857320\n",
      "diffuse_radiation_Wm2_roll_6    0.856514\n",
      "- Eto_mm:\n",
      "global_radiation_Wm2_lag_1     0.869796\n",
      "temp_rad_interaction_lag_1     0.864932\n",
      "global_radiation_Wm2_lag_2     0.851423\n",
      "temp_rad_interaction_lag_2     0.844836\n",
      "global_radiation_Wm2_lag_3     0.829700\n",
      "global_radiation_Wm2_roll_6    0.824138\n",
      "temp_rad_interaction_lag_3     0.821398\n",
      "temp_rad_interaction_roll_6    0.814385\n",
      "hour_cos_base                 -0.809493\n",
      "global_radiation_Wm2_lag_4     0.806088\n",
      "Top model feature importances:\n",
      "- air_temp_C:\n",
      "hour_cos_base                   0.162283\n",
      "temp_rad_interaction_lag_1      0.158650\n",
      "diffuse_radiation_Wm2_lag_2     0.109214\n",
      "diffuse_radiation_Wm2_lag_3     0.098257\n",
      "diffuse_radiation_Wm2_roll_6    0.047202\n",
      "weekofyear_roll_144             0.032510\n",
      "hour_lag_5                      0.029482\n",
      "air_temp_C_weather_lag_1        0.028431\n",
      "diffuse_radiation_Wm2_lag_1     0.027441\n",
      "temp_rad_interaction_lag_7      0.025050\n",
      "global_radiation_Wm2_lag_1      0.020479\n",
      "air_temp_C_weather_lag_2        0.017361\n",
      "temp_rad_interaction_lag_2      0.016800\n",
      "dayofyear_cos_roll_144          0.015637\n",
      "hour_roll_6                     0.012607\n",
      "- Eto_mm:\n",
      "hour_cos_base                        0.388938\n",
      "air_temp_C_weather_diff_roll_12      0.153285\n",
      "hour_lag_4                           0.028941\n",
      "temp_rad_interaction_lag_1           0.025620\n",
      "global_radiation_Wm2_lag_1           0.021558\n",
      "global_radiation_Wm2_lag_2           0.021019\n",
      "hour_lag_2                           0.019376\n",
      "global_radiation_Wm2_diff_roll_12    0.015727\n",
      "diffuse_radiation_Wm2_lag_1          0.015624\n",
      "temp_rad_interaction_lag_10          0.009827\n",
      "temp_rad_interaction_lag_2           0.009722\n",
      "hour_roll_6                          0.009398\n",
      "hour_lag_1                           0.008644\n",
      "temp_rad_interaction_lag_8           0.007818\n",
      "air_temp_C_weather_diff_roll_6       0.007641\n",
      "Exported air_temp_C test rows with predictions to C:\\Users\\edene\\OneDrive\\שולחן העבודה\\final_data_file\\test_rows_air_temp_C_predictions.csv\n",
      "Exported Eto_mm test rows with predictions to C:\\Users\\edene\\OneDrive\\שולחן העבודה\\final_data_file\\test_rows_eto_mm_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "metrics = {}\n",
    "models = {}\n",
    "predicted_series = {}\n",
    "diagnostics = {}\n",
    "search_results = {}\n",
    "\n",
    "def _train_with_search(X_train, y_train, target: str, val_frac: float = 0.05):\n",
    "    base_params = dict(\n",
    "        n_estimators=1200,\n",
    "        max_depth=3,\n",
    "        min_child_weight=50 if target == \"air_temp_C\" else 20,\n",
    "        learning_rate=0.03 if target == \"air_temp_C\" else 0.02,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.6,\n",
    "        reg_lambda=6.0 if target == \"air_temp_C\" else 3.0,\n",
    "    )\n",
    "    param_grid = [\n",
    "        dict(n_estimators=800, learning_rate=0.06, max_depth=2, min_child_weight=35, subsample=0.9, colsample_bytree=0.95, reg_alpha=0.2, reg_lambda=3.0),\n",
    "        dict(n_estimators=950, learning_rate=0.05, max_depth=3, min_child_weight=45, subsample=0.9, colsample_bytree=0.9, reg_alpha=0.4, reg_lambda=4.5),\n",
    "        dict(n_estimators=1100, learning_rate=0.04, max_depth=3, min_child_weight=55, subsample=0.8, colsample_bytree=0.85, reg_alpha=0.6, reg_lambda=4.5),\n",
    "        dict(n_estimators=650, learning_rate=0.08, max_depth=2, min_child_weight=25, subsample=0.95, colsample_bytree=0.95, reg_alpha=0.1, reg_lambda=2.5),\n",
    "        dict(n_estimators=750, learning_rate=0.07, max_depth=3, min_child_weight=50, subsample=0.9, colsample_bytree=0.9, reg_alpha=0.5, reg_lambda=5.0),\n",
    "        dict(n_estimators=1300, learning_rate=0.03, max_depth=3, min_child_weight=60, subsample=0.8, colsample_bytree=0.85, reg_alpha=0.8, reg_lambda=5.5),\n",
    "        dict(n_estimators=900, learning_rate=0.05, max_depth=4, min_child_weight=60, subsample=0.85, colsample_bytree=0.9, reg_alpha=0.7, reg_lambda=6.0),\n",
    "    ]\n",
    "\n",
    "    # If too few rows, skip search and fit once\n",
    "    if len(X_train) < 40:\n",
    "        model = XGBRegressor(\n",
    "            **base_params,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=-1,\n",
    "            random_state=SEED,\n",
    "            eval_metric=\"rmse\",\n",
    "        )\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        return model, {\"best_params\": base_params, \"val_r2\": np.nan, \"val_mae\": np.nan, \"used_search\": False}\n",
    "\n",
    "    split_val = int(len(X_train) * (1 - val_frac))\n",
    "    if split_val <= 0 or split_val >= len(X_train):\n",
    "        split_val = max(1, len(X_train) - 1)\n",
    "    X_tr, X_val = X_train.iloc[:split_val], X_train.iloc[split_val:]\n",
    "    y_tr, y_val = y_train.iloc[:split_val], y_train.iloc[split_val:]\n",
    "\n",
    "    best = {\"val_r2\": -np.inf, \"val_mae\": np.inf, \"params\": base_params}\n",
    "    grid = list(param_grid)\n",
    "    if target == \"air_temp_C\":\n",
    "        grid.append(dict(n_estimators=1000, learning_rate=0.05, max_depth=3, min_child_weight=50, subsample=0.85, colsample_bytree=0.9, reg_alpha=0.6, reg_lambda=5.5))\n",
    "        grid.append(dict(n_estimators=800, learning_rate=0.06, max_depth=2, min_child_weight=60, subsample=0.9, colsample_bytree=0.95, reg_alpha=0.4, reg_lambda=6.0))\n",
    "    for params in grid:\n",
    "        model = XGBRegressor(\n",
    "            **params,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=-1,\n",
    "            random_state=SEED,\n",
    "            eval_metric=\"rmse\",\n",
    "        )\n",
    "        model.fit(X_tr, y_tr, verbose=False)\n",
    "        pred_val = model.predict(X_val)\n",
    "        val_r2 = r2_score(y_val, pred_val)\n",
    "        val_mae = mean_absolute_error(y_val, pred_val)\n",
    "        if val_r2 > best[\"val_r2\"]:\n",
    "            best = {\"val_r2\": val_r2, \"val_mae\": val_mae, \"params\": params}\n",
    "\n",
    "    final_model = XGBRegressor(\n",
    "        **best[\"params\"],\n",
    "        objective=\"reg:squarederror\",\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=-1,\n",
    "        random_state=SEED,\n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "    final_model.fit(X_train, y_train, verbose=False)\n",
    "    return final_model, {\"best_params\": best[\"params\"], \"val_r2\": best.get(\"val_r2\", np.nan), \"val_mae\": best.get(\"val_mae\", np.nan), \"used_search\": True}\n",
    "\n",
    "target_defs = {\n",
    "    \"air_temp_C\": rh_eto[[TIMESTAMP_COL, \"air_temp_C\"]].rename(columns={\"air_temp_C\": \"target\"}),\n",
    "    \"Eto_mm\": rh_eto[[TIMESTAMP_COL, \"Eto_mm\"]].rename(columns={\"Eto_mm\": \"target\"}),\n",
    "}\n",
    "\n",
    "for target_col, df_target in target_defs.items():\n",
    "    merged = merge_target_with_weather(df_target, target_col=\"target\").rename(columns={\"target\": target_col})\n",
    "    feature_vars = outside_vars.copy()\n",
    "    if merged.empty:\n",
    "        metrics[target_col] = {\n",
    "            \"train_r2\": np.nan, \"train_mae\": np.nan,\n",
    "            \"test_r2\": np.nan, \"test_mae\": np.nan,\n",
    "            \"train_rows\": 0, \"test_rows\": 0,\n",
    "            \"val_r2\": np.nan, \"val_mae\": np.nan,\n",
    "        }\n",
    "        diagnostics[target_col] = {\n",
    "            \"correlations\": pd.Series(dtype=float),\n",
    "            \"feature_importances\": pd.Series(dtype=float),\n",
    "        }\n",
    "        continue\n",
    "    X, y, ts = build_lagged(merged[[TIMESTAMP_COL, target_col] + feature_vars], target_col, include_target_lags=False, feature_vars=feature_vars)\n",
    "    if X.empty:\n",
    "        metrics[target_col] = {\n",
    "            \"train_r2\": np.nan, \"train_mae\": np.nan,\n",
    "            \"test_r2\": np.nan, \"test_mae\": np.nan,\n",
    "            \"train_rows\": 0, \"test_rows\": 0,\n",
    "            \"val_r2\": np.nan, \"val_mae\": np.nan,\n",
    "        }\n",
    "        diagnostics[target_col] = {\n",
    "            \"correlations\": pd.Series(dtype=float),\n",
    "            \"feature_importances\": pd.Series(dtype=float),\n",
    "        }\n",
    "        continue\n",
    "    split_idx = int(len(ts) * (1 - TEST_FRAC))\n",
    "    cutoff_time = ts.iloc[split_idx]\n",
    "    train_mask = ts < cutoff_time\n",
    "    test_mask = ~train_mask\n",
    "    X_train, X_test = X.loc[train_mask], X.loc[test_mask]\n",
    "    y_train, y_test = y.loc[train_mask], y.loc[test_mask]\n",
    "\n",
    "    model, search_info = _train_with_search(X_train, y_train, target_col, val_frac=0.05)\n",
    "    search_results[target_col] = search_info\n",
    "\n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    metrics[target_col] = {\n",
    "        \"train_r2\": r2_score(y_train, pred_train),\n",
    "        \"train_mae\": mean_absolute_error(y_train, pred_train),\n",
    "        \"test_r2\": r2_score(y_test, pred_test),\n",
    "        \"test_mae\": mean_absolute_error(y_test, pred_test),\n",
    "        \"train_rows\": len(X_train),\n",
    "        \"test_rows\": len(X_test),\n",
    "        \"val_r2\": search_info.get(\"val_r2\", np.nan),\n",
    "        \"val_mae\": search_info.get(\"val_mae\", np.nan),\n",
    "    }\n",
    "    diagnostics[target_col] = {\n",
    "        \"correlations\": pd.concat([\n",
    "            y_train.reset_index(drop=True),\n",
    "            X_train.reset_index(drop=True),\n",
    "        ], axis=1).corr()[target_col].drop(target_col).sort_values(\n",
    "            key=lambda s: s.abs(), ascending=False\n",
    "        ),\n",
    "        \"feature_importances\": pd.Series(model.feature_importances_, index=X_train.columns).sort_values(ascending=False),\n",
    "    }\n",
    "    models[target_col] = model\n",
    "    predicted_series[target_col] = pd.DataFrame({\n",
    "        TIMESTAMP_COL: ts.reset_index(drop=True),\n",
    "        f\"pred_{target_col}\": model.predict(X.reset_index(drop=True)),\n",
    "    })\n",
    "\n",
    "print(\"Metrics (per target, train/test/val):\")\n",
    "def fmt(val):\n",
    "    return f\"{val:.4f}\" if pd.notnull(val) else \"nan\"\n",
    "for tgt, m in metrics.items():\n",
    "    print(\n",
    "        f\"- {tgt}: \"\n",
    "        f\"train R2={fmt(m.get('train_r2', np.nan))} MAE={fmt(m.get('train_mae', np.nan))} | \"\n",
    "        f\"test R2={fmt(m.get('test_r2', np.nan))} MAE={fmt(m.get('test_mae', np.nan))} | \"\n",
    "        f\"val R2={fmt(m.get('val_r2', np.nan))} MAE={fmt(m.get('val_mae', np.nan))} | \"\n",
    "        f\"train_rows={m.get('train_rows', 0)}, test_rows={m.get('test_rows', 0)}\"\n",
    "    )\n",
    "\n",
    "print(\"Top correlations with target (train set, abs sorted):\")\n",
    "for tgt, diag in diagnostics.items():\n",
    "    corr = diag.get(\"correlations\", pd.Series(dtype=float))\n",
    "    if corr.empty:\n",
    "        print(f\"- {tgt}: no correlation data\")\n",
    "        continue\n",
    "    print(f\"- {tgt}:\")\n",
    "    print(corr.head(10).to_string())\n",
    "\n",
    "print(\"Top model feature importances:\")\n",
    "for tgt, diag in diagnostics.items():\n",
    "    imp = diag.get(\"feature_importances\", pd.Series(dtype=float))\n",
    "    if imp.empty:\n",
    "        print(f\"- {tgt}: no importance data\")\n",
    "        continue\n",
    "    print(f\"- {tgt}:\")\n",
    "    print(imp.head(15).to_string())\n",
    "\n",
    "# Export test rows with predictions\n",
    "for tgt, fname in [\n",
    "    (\"air_temp_C\", \"test_rows_air_temp_C_predictions.csv\"),\n",
    "    (\"Eto_mm\", \"test_rows_eto_mm_predictions.csv\"),\n",
    "]:\n",
    "    if tgt not in target_defs or tgt not in models:\n",
    "        continue\n",
    "    merged = merge_target_with_weather(target_defs[tgt], target_col=\"target\").rename(columns={\"target\": tgt})\n",
    "    feature_vars = outside_vars.copy()\n",
    "    X_tmp, y_tmp, ts_tmp = build_lagged(merged[[TIMESTAMP_COL, tgt] + feature_vars], tgt, include_target_lags=False, feature_vars=feature_vars)\n",
    "    if X_tmp.empty:\n",
    "        continue\n",
    "    split_idx = int(len(ts_tmp) * (1 - TEST_FRAC))\n",
    "    cutoff_time = ts_tmp.iloc[split_idx]\n",
    "    test_mask = ts_tmp >= cutoff_time\n",
    "    X_test_tmp = X_tmp.loc[test_mask].reset_index(drop=True)\n",
    "    y_test_tmp = y_tmp.loc[test_mask].reset_index(drop=True)\n",
    "    pred_tmp = models[tgt].predict(X_test_tmp)\n",
    "    ts_test = ts_tmp.loc[test_mask].reset_index(drop=True)\n",
    "    export_df = pd.DataFrame({\n",
    "        TIMESTAMP_COL: ts_test,\n",
    "        tgt: y_test_tmp,\n",
    "        f\"pred_{tgt}\": pred_tmp,\n",
    "    })\n",
    "    export_path = DATA_DIR / fname\n",
    "    export_df.to_csv(export_path, index=False)\n",
    "    print(f\"Exported {tgt} test rows with predictions to {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c830dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rolling blocks for air_temp_C: R2=0.9464 MAE=0.7472 over 940 rows\n",
      " block               start                 end       r2      mae  rows\n",
      "     0 2025-08-15 16:00:00 2025-08-16 23:10:00 0.896770 1.195896   188\n",
      "     1 2025-08-25 04:30:00 2025-08-26 11:40:00 0.971219 0.440177   188\n",
      "     2 2025-08-30 09:50:00 2025-09-04 17:10:00 0.986193 0.466945   188\n",
      "     3 2025-09-08 15:20:00 2025-09-11 17:50:00 0.936264 0.787412   188\n",
      "     4 2025-09-15 16:00:00 2025-09-16 23:10:00 0.905369 0.845331   188\n",
      "Rolling blocks for Eto_mm: R2=0.9400 MAE=0.0273 over 1330 rows\n",
      " block               start                 end       r2      mae  rows\n",
      "     0 2025-08-16 13:10:00 2025-08-18 09:20:00 0.915948 0.035025   266\n",
      "     1 2025-08-23 22:40:00 2025-08-25 18:50:00 0.964661 0.024061   266\n",
      "     2 2025-08-31 08:10:00 2025-09-02 04:20:00 0.962090 0.023215   266\n",
      "     3 2025-09-07 17:40:00 2025-09-09 13:50:00 0.952157 0.022438   266\n",
      "     4 2025-09-15 03:10:00 2025-09-16 23:20:00 0.874397 0.031881   266\n"
     ]
    }
   ],
   "source": [
    "# Rolling blocked training/eval (train small slice, test next slice repeatedly)\n",
    "from math import ceil\n",
    "\n",
    "def rolling_block_eval(target: str, train_frac: float = 0.15, test_frac: float = 0.05):\n",
    "    if target not in target_defs:\n",
    "        raise ValueError(f\"Unknown target {target}\")\n",
    "    merged = merge_target_with_weather(target_defs[target], target_col=\"target\").rename(columns={\"target\": target})\n",
    "    X, y, ts = build_lagged(merged[[TIMESTAMP_COL, target] + outside_vars], target, include_target_lags=False, feature_vars=outside_vars)\n",
    "    n = len(X)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"No data\")\n",
    "    train_size = max(1, int(n * train_frac))\n",
    "    test_size = max(1, int(n * test_frac))\n",
    "    if train_size + test_size >= n:\n",
    "        raise ValueError(\"Fractions too large\")\n",
    "\n",
    "    blocks = []\n",
    "    preds_all, actual_all, ts_all = [], [], []\n",
    "    idx = 0\n",
    "    block_id = 0\n",
    "    while idx + train_size + test_size <= n:\n",
    "        train_end = idx + train_size\n",
    "        test_end = train_end + test_size\n",
    "        X_train, y_train = X.iloc[idx:train_end], y.iloc[idx:train_end]\n",
    "        X_test, y_test = X.iloc[train_end:test_end], y.iloc[train_end:test_end]\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=3,\n",
    "            min_child_weight=40,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.65,\n",
    "            colsample_bytree=0.65,\n",
    "            reg_alpha=1.0,\n",
    "            reg_lambda=4.0,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=-1,\n",
    "            random_state=SEED,\n",
    "            eval_metric=\"rmse\",\n",
    "        )\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        pred = model.predict(X_test)\n",
    "        preds_all.append(pred)\n",
    "        actual_all.append(y_test.to_numpy())\n",
    "        ts_all.append(ts.iloc[train_end:test_end])\n",
    "        blocks.append({\n",
    "            \"block\": block_id,\n",
    "            \"start\": ts.iloc[train_end],\n",
    "            \"end\": ts.iloc[test_end-1],\n",
    "            \"r2\": r2_score(y_test, pred),\n",
    "            \"mae\": mean_absolute_error(y_test, pred),\n",
    "            \"rows\": len(pred),\n",
    "        })\n",
    "        idx = test_end\n",
    "        block_id += 1\n",
    "\n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    actual_all = np.concatenate(actual_all)\n",
    "    ts_all = pd.concat(ts_all).reset_index(drop=True)\n",
    "    overall_r2 = r2_score(actual_all, preds_all)\n",
    "    overall_mae = mean_absolute_error(actual_all, preds_all)\n",
    "    print(f\"Rolling blocks for {target}: R2={overall_r2:.4f} MAE={overall_mae:.4f} over {len(actual_all)} rows\")\n",
    "    print(pd.DataFrame(blocks).to_string(index=False))\n",
    "    return pd.DataFrame({TIMESTAMP_COL: ts_all, target: actual_all, f\"pred_{target}_rolling\": preds_all})\n",
    "\n",
    "rolling_air = rolling_block_eval(\"air_temp_C\", train_frac=0.15, test_frac=0.05)\n",
    "rolling_eto = rolling_block_eval(\"Eto_mm\", train_frac=0.15, test_frac=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_full_weather",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict over full weather period and export ETo + internal air temp\n",
    "full_weather = outside.rename(columns={\"datetime\": TIMESTAMP_COL}).copy()\n",
    "export_targets = [\"air_temp_C\", \"Eto_mm\"]\n",
    "export_frames = []\n",
    "\n",
    "for tgt in export_targets:\n",
    "    if tgt not in models:\n",
    "        print(f\"Skipping {tgt}: no trained model\")\n",
    "        continue\n",
    "    weather_for_pred = full_weather.copy()\n",
    "    weather_for_pred[tgt] = 0.0\n",
    "    X_full, _, ts_full = build_lagged(\n",
    "        weather_for_pred[[TIMESTAMP_COL, tgt] + outside_vars],\n",
    "        tgt,\n",
    "        include_target_lags=False,\n",
    "        feature_vars=outside_vars,\n",
    "    )\n",
    "    if X_full.empty:\n",
    "        print(f\"Skipping {tgt}: no feature rows\")\n",
    "        continue\n",
    "    preds = models[tgt].predict(X_full)\n",
    "    export_frames.append(pd.DataFrame({\n",
    "        TIMESTAMP_COL: ts_full.reset_index(drop=True),\n",
    "        f\"pred_{tgt}\": preds,\n",
    "    }))\n",
    "\n",
    "if export_frames:\n",
    "    export_df = export_frames[0]\n",
    "    for df_extra in export_frames[1:]:\n",
    "        export_df = pd.merge(export_df, df_extra, on=TIMESTAMP_COL, how=\"inner\")\n",
    "    export_path = DATA_DIR / \"full_weather_period_air_temp_ETo_predictions.csv\"\n",
    "    export_df.to_csv(export_path, index=False)\n",
    "    print(f\"Exported full weather-period predictions to {export_path}\")\n",
    "else:\n",
    "    print(\"No predictions exported for full weather period\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab3ac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rolling forecast across full season (train on past window, predict next block, advance)\n",
    "def rolling_forecast_full(target: str, train_frac: float = 0.15, test_frac: float = 0.05):\n",
    "    if target not in target_defs:\n",
    "        raise ValueError(f\"Unknown target {target}\")\n",
    "    merged = merge_target_with_weather(target_defs[target], target_col=\"target\").rename(columns={\"target\": target})\n",
    "    X, y, ts = build_lagged(merged[[TIMESTAMP_COL, target] + outside_vars], target, include_target_lags=False, feature_vars=outside_vars)\n",
    "    n = len(X)\n",
    "    if n == 0:\n",
    "        raise ValueError(\"No data\")\n",
    "    train_size = max(1, int(n * train_frac))\n",
    "    test_size = max(1, int(n * test_frac))\n",
    "    if train_size + test_size >= n:\n",
    "        raise ValueError(\"Fractions too large for rolling forecast\")\n",
    "\n",
    "    preds_all, actual_all, ts_all = [], [], []\n",
    "    start = 0\n",
    "    block_id = 0\n",
    "    while True:\n",
    "        train_end = start + train_size\n",
    "        test_end = train_end + test_size\n",
    "        if test_end > n:\n",
    "            break\n",
    "        X_train, y_train = X.iloc[start:train_end], y.iloc[start:train_end]\n",
    "        X_test, y_test = X.iloc[train_end:test_end], y.iloc[train_end:test_end]\n",
    "        model = XGBRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=3,\n",
    "            min_child_weight=40,\n",
    "            learning_rate=0.02,\n",
    "            subsample=0.65,\n",
    "            colsample_bytree=0.65,\n",
    "            reg_alpha=1.0,\n",
    "            reg_lambda=4.0,\n",
    "            objective=\"reg:squarederror\",\n",
    "            tree_method=\"hist\",\n",
    "            n_jobs=-1,\n",
    "            random_state=SEED,\n",
    "            eval_metric=\"rmse\",\n",
    "        )\n",
    "        model.fit(X_train, y_train, verbose=False)\n",
    "        pred = model.predict(X_test)\n",
    "        preds_all.append(pred)\n",
    "        actual_all.append(y_test.to_numpy())\n",
    "        ts_all.append(ts.iloc[train_end:test_end])\n",
    "        print(f\"Block {block_id}: {ts.iloc[train_end]} -> {ts.iloc[test_end-1]} R2={r2_score(y_test, pred):.4f} MAE={mean_absolute_error(y_test, pred):.4f} rows={len(pred)}\")\n",
    "        start += test_size  # advance window by test block to include predicted slice in future training\n",
    "        block_id += 1\n",
    "\n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    actual_all = np.concatenate(actual_all)\n",
    "    ts_all = pd.concat(ts_all).reset_index(drop=True)\n",
    "    df_out = pd.DataFrame({\n",
    "        TIMESTAMP_COL: ts_all,\n",
    "        f\"{target}_actual\": actual_all,\n",
    "        f\"pred_{target}_rolling_forecast\": preds_all,\n",
    "    })\n",
    "    overall_r2 = r2_score(actual_all, preds_all)\n",
    "    overall_mae = mean_absolute_error(actual_all, preds_all)\n",
    "    print(f\"Rolling forecast {target}: R2={overall_r2:.4f} MAE={overall_mae:.4f} over {len(actual_all)} rows\")\n",
    "    export_path = DATA_DIR / f\"rolling_forecast_{target}.csv\"\n",
    "    df_out.to_csv(export_path, index=False)\n",
    "    print(f\"Saved stitched rolling forecast to {export_path}\")\n",
    "    return df_out\n",
    "\n",
    "rolling_forecast_air = rolling_forecast_full(\"air_temp_C\", train_frac=0.15, test_frac=0.05)\n",
    "rolling_forecast_eto = rolling_forecast_full(\"Eto_mm\", train_frac=0.15, test_frac=0.05)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
